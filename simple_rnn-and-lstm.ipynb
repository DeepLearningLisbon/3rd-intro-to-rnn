{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple RNN example\n",
    "---\n",
    "\n",
    "In this notebook, we'll se a simple (perhaps the most simple) way of using RNNs... For text classification.\n",
    "\n",
    "For it, you'll need to download the data of IMDB sentiment classification: http://ai.stanford.edu/~amaas/data/sentiment/. This set is small enough for us to do it locally in our computers. It also provides a train and a test set. For our purpose, we'll use the test set as development/validation set. \n",
    "\n",
    "A lot of code of the preprocess Pipeline for IMDB dataset is from https://github.com/nyu-mll/DS-GA-1011-Fall2017/blob/master/hw1/HW01-student.ipynb (I recommend you doing this excersises and labs on your own! I took this class and it was extremely good. Syllable here: https://docs.google.com/document/d/1SIPSt4aeB3Lys9ztCp47Y4v68R6Awt8NBTlCObp2njg/edit). \n",
    "\n",
    "To tokenize, we're using the simple nltk word tokenizer (https://www.nltk.org/api/nltk.tokenize.html), but feel free to try moses or any other tokenizer. To use it, you'll probably have to download nltk's data. \n",
    "\n",
    "Creating this notebook, I came across someone who did something similar:\n",
    "* https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. File params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data/aclImdb/\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "TRAIN_SIZE = 23000 \n",
    "VALIDATION_SIZE = 2000\n",
    "TEST_SIZE = 2000 # I'm just loading 2000 out of the 25,000 to be able to work locally. Feel free to increase to 25k.\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# These can be treated as a hyperparm. \n",
    "VOCAB_SIZE = 20000 \n",
    "BATCH_SIZE = 40\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "HIDDEN_DIM = 256\n",
    "EMBEDDING_DIM = 400\n",
    "MAX_LEN = 200\n",
    "\n",
    "# GPU use (I'll be asuming 1 device). This will be hard coded throughout the notebook: \n",
    "if torch.cuda.is_available():\n",
    "    USE_CUDA = True\n",
    "    DEFAULT_DEVICE = 'cuda:0'\n",
    "else:\n",
    "    DEFAULT_DEVICE = 'cpu'\n",
    "print(DEFAULT_DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-process data\n",
    "\n",
    "Here, we're just loading the data into memory and we'll tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDatum():\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test datum\n",
    "    - self.raw_text\n",
    "    - self.label: 0 neg, 1 pos\n",
    "    - self.file_name: dir for this datum\n",
    "    - self.tokens: list of tokens\n",
    "    - self.token_idx: index of each token in the text\n",
    "    \"\"\"\n",
    "    def __init__(self, raw_text, label, file_name):\n",
    "        self.raw_text = raw_text\n",
    "        self.label = label\n",
    "        self.file_name = file_name\n",
    "        self.tokens = self._set_tokens(raw_text)\n",
    "        self.tokens_idx = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def _set_tokens(raw_text):\n",
    "        return word_tokenize(raw_text)\n",
    "        \n",
    "    def set_token_idx(self, token2idx, unk_token):\n",
    "        tokens_idx = []\n",
    "        for token in self.tokens:\n",
    "            token_idx = token2idx.get(token, token2idx[unk_token])\n",
    "            tokens_idx.append(token_idx)\n",
    "\n",
    "        self.tokens_idx = tokens_idx\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Function that cleans the string\n",
    "    \"\"\"\n",
    "    text = text.lower().replace(\"<br />\", \"\")\n",
    "    return text\n",
    "        \n",
    "    \n",
    "def read_file_as_datum(file_name, label):\n",
    "    \"\"\"\n",
    "    Function that reads a file \n",
    "    \"\"\"\n",
    "    with open(file_name, \"r\") as f:\n",
    "        content = f.read()\n",
    "        content = preprocess_text(content)\n",
    "    return IMDBDatum(raw_text=content, label=label, file_name=file_name)\n",
    "\n",
    "\n",
    "def construct_dataset(dataset_dir, dataset_size, offset=0):\n",
    "    \"\"\"\n",
    "    Function that loads a dataset\n",
    "    \"\"\"\n",
    "    pos_dir = os.path.join(dataset_dir, \"pos\")\n",
    "    neg_dir = os.path.join(dataset_dir, \"neg\")\n",
    "    single_label_size = int(dataset_size / 2)\n",
    "    output = []\n",
    "    all_pos = os.listdir(pos_dir)\n",
    "    all_neg = os.listdir(neg_dir)\n",
    "    for i in range(offset, offset+single_label_size):\n",
    "        output.append(read_file_as_datum(os.path.join(pos_dir, all_pos[i]), 1))\n",
    "        output.append(read_file_as_datum(os.path.join(neg_dir, all_neg[i]), 0))\n",
    "    return output\n",
    "\n",
    "def filter_dataum_dataset(dataset, max_len):\n",
    "    new_output = []\n",
    "    removed_samples = 0\n",
    "    total_samples = len(dataset)\n",
    "    for sample in dataset:\n",
    "        if len(sample.tokens) > max_len:\n",
    "            removed_samples += 1\n",
    "            continue\n",
    "        new_output.append(sample)\n",
    "            \n",
    "    print('Removed {} samples, thats {}% of set'.format(removed_samples, 100*round(removed_samples/total_samples, 2)))\n",
    "    print('Total samples: {}'.format(len(new_output)))\n",
    "    return new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working in train set...\n",
      "Removed 11291 samples, thats 49.0% of set\n",
      "Total samples: 11709\n",
      "Working in dev set...\n",
      "Removed 980 samples, thats 49.0% of set\n",
      "Total samples: 1020\n",
      "Working in test set...\n",
      "Removed 966 samples, thats 48.0% of set\n",
      "Total samples: 1034\n"
     ]
    }
   ],
   "source": [
    "print('Working in train set...')\n",
    "train_set = construct_dataset(train_dir, TRAIN_SIZE)\n",
    "train_set = filter_dataum_dataset(train_set, MAX_LEN)\n",
    "print('Working in dev set...')\n",
    "validation_set = construct_dataset(train_dir, VALIDATION_SIZE, offset=int(TRAIN_SIZE/2))\n",
    "validation_set = filter_dataum_dataset(validation_set, MAX_LEN)\n",
    "print('Working in test set...')\n",
    "test_set = construct_dataset(test_dir, TEST_SIZE)\n",
    "test_set = filter_dataum_dataset(test_set, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['broad', 'enough', 'for', 'you', '?', 'wait', 'till', 'you', 'see', 'this']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'film', 'well', '.', 'not', 'a', 'lot', 'of', 'fun', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].tokens[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature engineering\n",
    "\n",
    "Decide what features we should use (maybe tokenization decision should be here?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_token2idx(train_set, vocab_size=VOCAB_SIZE):\n",
    "    \"\"\"\n",
    "    Function that loads the train set and return a dict that maps tokens to index. \n",
    "    Indexes 0 and 1 are reserved for Padding and Unknown tokens respectively. \n",
    "    Currently hard coded. \n",
    "    \"\"\"\n",
    "    tokens_counter = Counter()\n",
    "    for datum in train_set:\n",
    "        for token in datum.tokens:\n",
    "            tokens_counter[token] += 1\n",
    "            \n",
    "    print('Number of unique tokens in train data: {}'.format(len(tokens_counter)))\n",
    "    print('Subsetting to: {}'.format(vocab_size))\n",
    "    top_k_tokens = tokens_counter.most_common(vocab_size) # This return a list of touples, not a Counter() object.\n",
    "    token2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for token_touples in top_k_tokens:\n",
    "        token = token_touples[0]\n",
    "        if token in token2idx.keys():\n",
    "            continue\n",
    "        \n",
    "        token2idx[token] = len(token2idx) + 1\n",
    "        \n",
    "    return token2idx\n",
    "\n",
    "def build_idx2tokens(token2idx):\n",
    "    \"\"\"\n",
    "    Function to build a dictionary that maps indexes to tokens from the reverse maps/\n",
    "    \"\"\"\n",
    "    return {v:k for k,v in token2idx.items()}\n",
    "\n",
    "def set_tokens_idx_in_datum(data_set, token2idx, unk_token='<UNK>'):\n",
    "    \"\"\"\n",
    "    Function to set the tokens in the list of datums.\n",
    "    \"\"\"\n",
    "    for datum in data_set:\n",
    "        datum.set_token_idx(token2idx, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in train data: 55344\n",
      "Subsetting to: 20000\n"
     ]
    }
   ],
   "source": [
    "token2idx = build_token2idx(train_set)\n",
    "idx2tokens = build_idx2tokens(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_tokens_idx_in_datum(train_set, token2idx)\n",
    "set_tokens_idx_in_datum(validation_set, token2idx)\n",
    "set_tokens_idx_in_datum(test_set, token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broad\n",
      "enough\n",
      "for\n",
      "you\n",
      "?\n",
      "wait\n",
      "till\n",
      "you\n",
      "see\n",
      "this\n"
     ]
    }
   ],
   "source": [
    "# Sanity check (ALWAYS DO SANITY CHECKS!!! It's so easy to screw up in this steps)\n",
    "for x in train_set[0].tokens_idx[0:10]:\n",
    "    print(idx2tokens[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check (ALWAYS DO SANITY CHECKS!!! It's so easy to screw up in this steps)\n",
    "# idx2tokens\n",
    "# token2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pytorch pipeline\n",
    "\n",
    "Build the pytorch pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    See the documentation of pytorch's dataloaders here: \n",
    "    https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of IMDBDatum\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        returns (tokens idxs list, len_of_sequence), label\n",
    "        \"\"\"\n",
    "        tokens_idx, label = self.data_list[key].tokens_idx, self.data_list[key].label\n",
    "        return (tokens_idx, len(tokens_idx)), label\n",
    "    \n",
    "\n",
    "\n",
    "def imdb_collate_and_force_len(batch, max_length=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length. \n",
    "    \n",
    "    PAD token is hard coded (0).\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    \n",
    "    # Padd the sequence (using numpy)\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[1])\n",
    "        length_list.append(datum[0][1])\n",
    "        padded_vec = np.pad(np.array(datum[0][0]), \n",
    "                            pad_width=((0,max_length-datum[0][1])), \n",
    "                            mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)).to(DEFAULT_DEVICE), torch.LongTensor(length_list).to(DEFAULT_DEVICE), torch.LongTensor(label_list).to(DEFAULT_DEVICE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an training sample: ([5185, 226, 19, 21, 61, 597, 1732, 21, 65, 13, 1323, 1, 8, 6, 132, 1, 34, 535, 4, 59, 10, 1469, 7, 1351, 1, 1499, 1541, 81, 144, 68, 5, 7, 13, 10, 2816, 4, 9, 221, 29, 1, 1779, 23, 1, 379, 46, 10, 25, 1469, 1, 56, 130, 5, 12, 18, 271, 8, 272, 4, 3, 2958, 30, 1, 5, 814, 37, 41, 137, 170, 5, 20, 12, 18, 664, 6, 336, 15, 1, 81, 9, 22, 85, 4, 27, 6, 167, 8, 211, 4], 88)\n",
      "This is a label: 0\n"
     ]
    }
   ],
   "source": [
    "# consturct datasets\n",
    "imdb_train = IMDBDataset(train_set)\n",
    "imdb_validation = IMDBDataset(validation_set)\n",
    "imdb_test = IMDBDataset(test_set)    \n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(dataset=imdb_train, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_collate_and_force_len,\n",
    "                                           shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=imdb_validation, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=imdb_collate_and_force_len,\n",
    "                                           shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=imdb_test, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_collate_and_force_len,\n",
    "                                           shuffle=False)\n",
    "\n",
    "print(\"This is an training sample: {0}\".format(imdb_train[0][0]))\n",
    "print(\"This is a label: {0}\".format(imdb_train[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple RNN model!!\n",
    "\n",
    "Build our very own RNN model!\n",
    "\n",
    "Check `models.py` to see specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OurAwsomeRNN(\n",
       "  (embedding): Embedding(20003, 400, padding_idx=0)\n",
       "  (rnn): RNN(400, 256, batch_first=True)\n",
       "  (linear): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OurAwsomeRNN(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, DEFAULT_DEVICE)\n",
    "# model = OurAwsomeLSTMWithTwoLayers(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, DEFAULT_DEVICE)\n",
    "# model = OurAwsomeRNNWithAllConnections(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, DEFAULT_DEVICE, MAX_LEN)\n",
    "model.to(DEFAULT_DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our loss function\n",
    "# See all of pytorch loss functions: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Define our optimizer:\n",
    "# See all of pytorch's optimizers: https://pytorch.org/docs/stable/optim.html\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch = Variable(data)\n",
    "        outputs = model(data)\n",
    "        predicted = (outputs.data > 0.5).long().view(-1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().cpu().numpy()\n",
    "    model.train()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/20], Step: [80/292], Loss: 0.6938442587852478, Train Acc: 49.210009394482874, Validation Acc:49.411764705882355\n",
      "Epoch: [1/20], Step: [160/292], Loss: 0.6875297427177429, Train Acc: 49.63703134341105, Validation Acc:50.294117647058826\n",
      "Epoch: [1/20], Step: [240/292], Loss: 0.6956914663314819, Train Acc: 50.08113417029635, Validation Acc:49.509803921568626\n",
      "Epoch: [2/20], Step: [80/292], Loss: 0.7686129808425903, Train Acc: 50.1409172431463, Validation Acc:50.19607843137255\n",
      "Epoch: [2/20], Step: [160/292], Loss: 0.6821452379226685, Train Acc: 50.064053292339224, Validation Acc:49.90196078431372\n",
      "Epoch: [2/20], Step: [240/292], Loss: 0.6980366706848145, Train Acc: 49.99572978051072, Validation Acc:49.11764705882353\n",
      "Epoch: [3/20], Step: [80/292], Loss: 0.6811031103134155, Train Acc: 49.88470407378939, Validation Acc:50.09803921568628\n",
      "Epoch: [3/20], Step: [160/292], Loss: 0.7068578600883484, Train Acc: 50.27756426680332, Validation Acc:49.6078431372549\n",
      "Epoch: [3/20], Step: [240/292], Loss: 0.6930145025253296, Train Acc: 50.03843197540353, Validation Acc:48.8235294117647\n",
      "Epoch: [4/20], Step: [80/292], Loss: 0.6879122257232666, Train Acc: 49.90178495174652, Validation Acc:49.6078431372549\n",
      "Epoch: [4/20], Step: [160/292], Loss: 0.6904600262641907, Train Acc: 50.35442821761038, Validation Acc:50.0\n",
      "Epoch: [4/20], Step: [240/292], Loss: 0.7020062804222107, Train Acc: 50.19215987701768, Validation Acc:50.490196078431374\n",
      "Epoch: [5/20], Step: [80/292], Loss: 0.6918405890464783, Train Acc: 50.48253480228884, Validation Acc:49.90196078431372\n",
      "Epoch: [5/20], Step: [160/292], Loss: 0.6961542963981628, Train Acc: 50.45691348535315, Validation Acc:50.19607843137255\n",
      "Epoch: [5/20], Step: [240/292], Loss: 0.7114766240119934, Train Acc: 50.516696558203094, Validation Acc:51.96078431372549\n",
      "Epoch: [6/20], Step: [80/292], Loss: 0.6509131193161011, Train Acc: 50.721667093688616, Validation Acc:51.372549019607845\n",
      "Epoch: [6/20], Step: [160/292], Loss: 0.6748303174972534, Train Acc: 50.74728841062431, Validation Acc:51.27450980392157\n",
      "Epoch: [6/20], Step: [240/292], Loss: 0.6910142302513123, Train Acc: 50.02989153642497, Validation Acc:49.31372549019608\n",
      "Epoch: [7/20], Step: [80/292], Loss: 0.7047078013420105, Train Acc: 49.42352036894697, Validation Acc:48.627450980392155\n",
      "Epoch: [7/20], Step: [160/292], Loss: 0.7013991475105286, Train Acc: 49.44060124690409, Validation Acc:48.72549019607843\n",
      "Epoch: [7/20], Step: [240/292], Loss: 0.689182460308075, Train Acc: 50.81561192245282, Validation Acc:51.470588235294116\n",
      "Epoch: [8/20], Step: [80/292], Loss: 0.6933127641677856, Train Acc: 50.03843197540353, Validation Acc:49.31372549019608\n",
      "Epoch: [8/20], Step: [160/292], Loss: 0.7000298500061035, Train Acc: 50.64480314288154, Validation Acc:51.666666666666664\n",
      "Epoch: [8/20], Step: [240/292], Loss: 0.6897161602973938, Train Acc: 50.021351097446406, Validation Acc:49.31372549019608\n",
      "Epoch: [9/20], Step: [80/292], Loss: 0.7128300070762634, Train Acc: 50.448373046374584, Validation Acc:51.666666666666664\n",
      "Epoch: [9/20], Step: [160/292], Loss: 0.6955469250679016, Train Acc: 49.56016739260398, Validation Acc:48.431372549019606\n",
      "Epoch: [9/20], Step: [240/292], Loss: 0.6927312612533569, Train Acc: 49.57724827056111, Validation Acc:48.529411764705884\n",
      "Epoch: [10/20], Step: [80/292], Loss: 0.7061956524848938, Train Acc: 50.70458621573149, Validation Acc:51.568627450980394\n",
      "Epoch: [10/20], Step: [160/292], Loss: 0.6927726864814758, Train Acc: 50.03843197540353, Validation Acc:49.31372549019608\n",
      "Epoch: [10/20], Step: [240/292], Loss: 0.6983979344367981, Train Acc: 50.64480314288154, Validation Acc:51.568627450980394\n",
      "Epoch: [11/20], Step: [80/292], Loss: 0.6958656311035156, Train Acc: 50.63626270390298, Validation Acc:51.666666666666664\n",
      "Epoch: [11/20], Step: [160/292], Loss: 0.7141634225845337, Train Acc: 50.71312665471005, Validation Acc:51.27450980392157\n",
      "Epoch: [11/20], Step: [240/292], Loss: 0.6963586211204529, Train Acc: 49.64557178238962, Validation Acc:48.431372549019606\n",
      "Epoch: [12/20], Step: [80/292], Loss: 0.7638794183731079, Train Acc: 50.6789648987958, Validation Acc:51.666666666666664\n",
      "Epoch: [12/20], Step: [160/292], Loss: 0.6785283088684082, Train Acc: 49.6028695874968, Validation Acc:48.627450980392155\n",
      "Epoch: [12/20], Step: [240/292], Loss: 0.6950305700302124, Train Acc: 49.67973353830387, Validation Acc:48.13725490196079\n",
      "Epoch: [13/20], Step: [80/292], Loss: 0.748756468296051, Train Acc: 49.56016739260398, Validation Acc:48.13725490196079\n",
      "Epoch: [13/20], Step: [160/292], Loss: 0.6892921328544617, Train Acc: 50.696045776752925, Validation Acc:52.15686274509804\n",
      "Epoch: [13/20], Step: [240/292], Loss: 0.6938193440437317, Train Acc: 50.5850200700316, Validation Acc:51.96078431372549\n",
      "Epoch: [14/20], Step: [80/292], Loss: 0.6889575719833374, Train Acc: 50.57647963105303, Validation Acc:52.05882352941177\n",
      "Epoch: [14/20], Step: [160/292], Loss: 0.7048547267913818, Train Acc: 49.551626953625416, Validation Acc:47.84313725490196\n",
      "Epoch: [14/20], Step: [240/292], Loss: 0.6817927360534668, Train Acc: 49.67119309932531, Validation Acc:47.745098039215684\n",
      "Epoch: [15/20], Step: [80/292], Loss: 0.6895917654037476, Train Acc: 50.721667093688616, Validation Acc:51.76470588235294\n",
      "Epoch: [15/20], Step: [160/292], Loss: 0.69243985414505, Train Acc: 49.816380561960884, Validation Acc:48.03921568627451\n",
      "Epoch: [15/20], Step: [240/292], Loss: 0.7054885029792786, Train Acc: 50.670424459817234, Validation Acc:51.76470588235294\n",
      "Epoch: [16/20], Step: [80/292], Loss: 0.687132716178894, Train Acc: 49.88470407378939, Validation Acc:48.03921568627451\n",
      "Epoch: [16/20], Step: [160/292], Loss: 0.6895437240600586, Train Acc: 49.87616363481083, Validation Acc:48.13725490196079\n",
      "Epoch: [16/20], Step: [240/292], Loss: 0.6981922388076782, Train Acc: 50.66188402083867, Validation Acc:51.86274509803921\n",
      "Epoch: [17/20], Step: [80/292], Loss: 0.6803684830665588, Train Acc: 50.19215987701768, Validation Acc:49.01960784313726\n",
      "Epoch: [17/20], Step: [160/292], Loss: 0.6855400800704956, Train Acc: 50.021351097446406, Validation Acc:47.450980392156865\n",
      "Epoch: [17/20], Step: [240/292], Loss: 0.6863275766372681, Train Acc: 49.97010846357503, Validation Acc:47.450980392156865\n",
      "Epoch: [18/20], Step: [80/292], Loss: 0.6900783777236938, Train Acc: 50.696045776752925, Validation Acc:52.450980392156865\n",
      "Epoch: [18/20], Step: [160/292], Loss: 0.6963524222373962, Train Acc: 50.27756426680332, Validation Acc:49.11764705882353\n",
      "Epoch: [18/20], Step: [240/292], Loss: 0.7128264904022217, Train Acc: 50.5850200700316, Validation Acc:53.333333333333336\n",
      "Epoch: [19/20], Step: [80/292], Loss: 0.6952968835830688, Train Acc: 50.602100947988724, Validation Acc:53.431372549019606\n",
      "Epoch: [19/20], Step: [160/292], Loss: 0.7200864553451538, Train Acc: 50.5850200700316, Validation Acc:53.13725490196079\n",
      "Epoch: [19/20], Step: [240/292], Loss: 0.7026783227920532, Train Acc: 49.97010846357503, Validation Acc:46.568627450980394\n",
      "Epoch: [20/20], Step: [80/292], Loss: 0.7108569145202637, Train Acc: 50.6789648987958, Validation Acc:53.13725490196079\n",
      "Epoch: [20/20], Step: [160/292], Loss: 0.7195045351982117, Train Acc: 50.68750533777436, Validation Acc:53.333333333333336\n",
      "Epoch: [20/20], Step: [240/292], Loss: 0.6841971278190613, Train Acc: 50.37150909556751, Validation Acc:47.745098039215684\n",
      "Maximum in checkpoint: 53.431372549019606\n"
     ]
    }
   ],
   "source": [
    "validation_acc_history = []\n",
    "stop_training = False\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        data_batch = Variable(data)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch) \n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % (BATCH_SIZE*2) == 0:\n",
    "            train_acc = test_model(train_loader, model)\n",
    "            val_acc = test_model(validation_loader, model)\n",
    "            print('Epoch: [{0}/{1}], Step: [{2}/{3}], Loss: {4}, Train Acc: {5}, Validation Acc:{6}'.format( \n",
    "                   epoch+1, NUM_EPOCHS, i+1, len(imdb_train)//BATCH_SIZE, loss.data.item(), \n",
    "                   train_acc, val_acc))\n",
    "            validation_acc_history.append(val_acc)\n",
    "            \n",
    "        # TODO: Implement early stopping or model selection based on Validation Acc!!!\n",
    "print('Maximum in checkpoint: {}'.format(np.max(validation_acc_history)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Evaluate based on best model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meetup",
   "language": "python",
   "name": "meetup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
